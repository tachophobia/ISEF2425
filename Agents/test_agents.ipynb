{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from framework import StandardScaler\n",
    "from dql import DQLAgent, DQLTrainer\n",
    "from ddpg import DDPGAgent, DDPGTrainer, LichtenbergAgent\n",
    "\n",
    "from PIL import Image\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "def to_gif(matrices, filepath, duration=25):\n",
    "    frames = []\n",
    "    for matrix in matrices:\n",
    "        image = Image.fromarray(matrix)\n",
    "        frames.append(image)\n",
    "    frames[0].save(filepath, save_all=True, append_images=frames[1:], duration=duration, loop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submarine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments.submarine import ContinuousSubmarine\n",
    "import time\n",
    "\n",
    "episodes = []\n",
    "time_taken = []\n",
    "env = ContinuousSubmarine(delta_t=1, randomize=False) \n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "featurizer = StandardScaler(state_dim, learn=False)\n",
    "featurizer.mean = np.array([ 7.73739964, 11.54841878,  1.6379028 ,  1.92430292])\n",
    "featurizer.var = np.array([ 580.12531577, 1460.05771442,   36.57866531,   37.08183991])\n",
    "# for training comparison stability, ensure that the featurizer scaling is constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.env.delta_t = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def to_gif(matrices, filepath, duration=25):\n",
    "    frames = []\n",
    "    for matrix in matrices:\n",
    "        image = Image.fromarray(matrix)\n",
    "        frames.append(image)\n",
    "    frames[0].save(filepath, save_all=True, append_images=frames[1:], duration=duration, loop=0)\n",
    "\n",
    "# Initialize simulation\n",
    "state, info = trainer.env.reset()\n",
    "state = trainer.featurizer.transform_state(state, info)\n",
    "frames = []\n",
    "q_frames = []\n",
    "actions = []\n",
    "total_reward = 0.0\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action = trainer.agent.act(state, explore=False)\n",
    "    a = trainer.featurizer.transform_action(action, trainer.env.action_space.low, trainer.env.action_space.high)\n",
    "    actions.append(a)\n",
    "    next_state, reward, terminated, truncated, info = trainer.env.step(a)\n",
    "    done = terminated or truncated\n",
    "    total_reward += reward\n",
    "    state = trainer.featurizer.transform_state(next_state, info)\n",
    "    \n",
    "    # Render environment frame\n",
    "    env_frame = trainer.env.render()\n",
    "\n",
    "    trainer.agent.q_approximator.state = trainer.featurizer.transform_state(trainer.env.state)\n",
    "    \n",
    "    # Generate Q-approximator plot\n",
    "    x = np.linspace(trainer.agent.q_approximator.lower_bound[0], trainer.agent.q_approximator.upper_bound[0], 50)\n",
    "    y = np.linspace(trainer.agent.q_approximator.lower_bound[1], trainer.agent.q_approximator.upper_bound[1], 50)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = np.fromiter((trainer.agent.q_approximator.evaluate(np.array([x, y])) for x, y in zip(np.ravel(X), np.ravel(Y))), dtype=float, count=X.size).reshape(X.shape)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='viridis', alpha=0.7)\n",
    "    ax.set_xlabel('$a_0$ (angle)')\n",
    "    ax.set_ylabel('$a_1$ (magnitude)')\n",
    "    ax.set_title(f'$Q(s_t, a)$ for timestep')\n",
    "    \n",
    "    # Convert Q-approximator plot to RGB array\n",
    "    fig.canvas.draw()\n",
    "    q_frame = np.array(fig.canvas.renderer.buffer_rgba())\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Resize Q-approximator plot to be smaller\n",
    "    scale = 0.45\n",
    "    f1, f0 = env_frame.shape[1] * scale, env_frame.shape[0] * scale\n",
    "    q_frame = Image.fromarray(q_frame).resize((int(f1), int(f0)))\n",
    "    q_frame = np.array(q_frame)\n",
    "    q_frames.append(q_frame)\n",
    "    \n",
    "    # Convert environment frame to PIL image\n",
    "    env_img = Image.fromarray(env_frame)\n",
    "    \n",
    "    # Create blank canvas\n",
    "    combined_img = Image.new(\"RGB\", (env_img.width, env_img.height), (255, 255, 255))\n",
    "    combined_img.paste(env_img, (0, 0))\n",
    "    q_frame = q_frame[20:-30, 80:-60, :]\n",
    "    combined_img.paste(Image.fromarray(q_frame), (130, env_img.height - q_frame.shape[0] - 90))  # Place Q-approximator in top right\n",
    "    \n",
    "    frames.append(np.array(combined_img))\n",
    "    \n",
    "    # Stop when x threshold is passed\n",
    "    if trainer.env.state[1] > 15:\n",
    "        break\n",
    "\n",
    "gif_path = 'simulation_q_function.gif'\n",
    "to_gif(frames, gif_path, duration=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Generate continuation GIF with smoother translation and scaling of the Q-approximator animation\n",
    "last_frame = frames[-1]\n",
    "paused_frames = [last_frame] * 30  # Pause effect\n",
    "\n",
    "# Define the starting and ending positions and scales\n",
    "start_scale = 0.45\n",
    "end_scale = 1.0\n",
    "start_position = (130, env_img.height - q_frames[-1].shape[0] - 90)  # Top-right corner\n",
    "end_position = ((env_img.width - int(env_frame.shape[1] * end_scale)) // 2,\n",
    "                (env_img.height - int(env_frame.shape[0] * end_scale)) // 2)  # Center\n",
    "\n",
    "# Number of frames for the transition\n",
    "num_transition_frames = 40\n",
    "\n",
    "# Easing function: Slow at the start, fast in the middle, slow at the end\n",
    "def ease_in_out_sine(t):\n",
    "    return -(math.cos(math.pi * t) - 1) / 2\n",
    "\n",
    "# Generate smooth scaling and translation frames with easing\n",
    "scaling_frames = []\n",
    "for i in range(1, num_transition_frames + 1):\n",
    "    # Calculate eased progress (t) using the easing function\n",
    "    t = ease_in_out_sine(i / num_transition_frames)\n",
    "\n",
    "    # Interpolate scale\n",
    "    scale_factor = start_scale + (end_scale - start_scale) * t\n",
    "\n",
    "    # Interpolate position\n",
    "    x_pos = start_position[0] + (end_position[0] - start_position[0]) * t\n",
    "    y_pos = start_position[1] + (end_position[1] - start_position[1]) * t\n",
    "\n",
    "    # Resize Q-approximator plot\n",
    "    resized_q_frame = Image.fromarray(q_frames[-1]).resize(\n",
    "        (int(env_frame.shape[1] * scale_factor), int(env_frame.shape[0] * scale_factor))\n",
    "    )\n",
    "    resized_q_frame = np.array(resized_q_frame)\n",
    "\n",
    "    # Create new frame with translated and scaled Q-approximator\n",
    "    new_frame = Image.new(\"RGB\", (env_img.width, env_img.height), (255, 255, 255))\n",
    "    new_frame.paste(env_img, (0, 0))\n",
    "    new_frame.paste(Image.fromarray(resized_q_frame), (int(x_pos), int(y_pos)))\n",
    "\n",
    "    scaling_frames.append(np.array(new_frame))\n",
    "\n",
    "# Save the continuation GIF\n",
    "gif_cont_path = 'q_function_transition.gif'\n",
    "to_gif(paused_frames + scaling_frames + 30 * [scaling_frames[-1]], gif_cont_path, duration=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LichtenbergFigure:\n",
    "    def __init__(self, grid, center, bounds):\n",
    "        self.grid = grid\n",
    "        self.Rc = grid.shape[0] // 2\n",
    "        self.center = center\n",
    "        self.bounds = bounds\n",
    "\n",
    "        self.trigger = self.center\n",
    "        self.lower, self.upper = self.bounds\n",
    "\n",
    "        self.scale = 1\n",
    "        self.rot = 0\n",
    "\n",
    "    def sample(self, n):\n",
    "        flat = self.grid.flatten()\n",
    "        indices = np.random.choice(len(flat), p=flat/np.sum(flat), size=n)\n",
    "        coords = []\n",
    "        factor = (self.upper - self.lower) / (self.Rc * 2)\n",
    "        for idx in indices:\n",
    "            coord = np.unravel_index(idx, self.grid.shape)\n",
    "            # 2d rotate the coord about the center (Rc, Rc)\n",
    "            coord = np.array(coord) - self.Rc\n",
    "            coord = np.dot(coord, [[np.cos(self.rot), -np.sin(self.rot)], [np.sin(self.rot), np.cos(self.rot)]]) + self.Rc # 2d rotation\n",
    "            # scale the coord about (Rc, Rc)\n",
    "            coord = (coord - self.Rc) * self.scale + self.Rc\n",
    "            coord = factor * coord + self.lower + self.trigger - self.center\n",
    "            coords.append(coord)\n",
    "        \n",
    "        return coords\n",
    "    \n",
    "    def rand_transform(self, trigger):\n",
    "        self.scale = np.random.uniform(0.01, 1)\n",
    "        self.rot = np.random.uniform(0, 2*np.pi)\n",
    "        self.trigger = trigger\n",
    "\n",
    "    def copy(self, ref=1.):\n",
    "        copy = LichtenbergFigure(self.grid.copy(), self.center, self.bounds)\n",
    "        copy.scale = self.scale * ref\n",
    "        copy.rot = self.rot\n",
    "        copy.trigger = self.trigger\n",
    "        return copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the optimization process\n",
    "J = trainer.agent.q_approximator  # Replace with your Q-approximator\n",
    "n_iter = 3\n",
    "pop = 15\n",
    "\n",
    "figures = []\n",
    "max_samples = []\n",
    "sampled_points = []\n",
    "\n",
    "trigger = J.center() + np.random.normal(0, 0.1, 2)\n",
    "best_coords = trigger\n",
    "best_fitness = J.evaluate(trigger)\n",
    "\n",
    "lf = LichtenbergFigure(np.load('figure2d.npy'), trigger, J.bounds())\n",
    "\n",
    "# Define constant z_lim\n",
    "x = np.linspace(J.lower_bound[0], J.upper_bound[0], 200)\n",
    "y = np.linspace(J.lower_bound[1], J.upper_bound[1], 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.array([J.evaluate(np.array([x, y])) for x, y in zip(X.flatten(), Y.flatten())]).reshape(X.shape)\n",
    "min_z = np.min(Z.flatten())\n",
    "max_z = np.max(Z.flatten())\n",
    "\n",
    "frames = []\n",
    "best_coords_history = []  # To store all best coordinates\n",
    "\n",
    "for it in range(n_iter):\n",
    "    lf.rand_transform(trigger)\n",
    "    samples = lf.sample(pop)\n",
    "    sampled_points.extend(samples)\n",
    "    figures.append(lf.sample(10000))  # For figure display\n",
    "\n",
    "    # Animate the Lichtenberg figure by expanding radius\n",
    "    radius_steps = 20\n",
    "    for r in np.linspace(0, 1, radius_steps):\n",
    "        fig = plt.figure(figsize=(10, 8), dpi=200)\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "        # Plot the Q-approximator surface\n",
    "        ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='viridis', alpha=0.7, edgecolor='none')\n",
    "\n",
    "        # Plot the Lichtenberg figure with expanding radius\n",
    "        figure = np.array(figures[-1])\n",
    "        mask = np.linalg.norm(figure - trigger, axis=1) <= r * np.max(np.linalg.norm(figure - trigger, axis=1))\n",
    "        figure_x = figure[mask, 0]\n",
    "        figure_y = figure[mask, 1]\n",
    "        figure_z = [min_z for _ in figure[mask]]\n",
    "        ax.scatter(figure_x, figure_y, figure_z, color='black', alpha=0.5, s=0.1)\n",
    "\n",
    "        # Plot all previous best points in red\n",
    "        if best_coords_history:\n",
    "            best_x = [p[0] for p in best_coords_history]\n",
    "            best_y = [p[1] for p in best_coords_history]\n",
    "            best_z = [J.evaluate(p) for p in best_coords_history]\n",
    "            ax.scatter(best_x, best_y, best_z, color='red', s=100, edgecolors='black', label='Best Points')\n",
    "            ax.plot(best_x, best_y, best_z, color='red', linestyle='--', label='Best Path')\n",
    "\n",
    "        # Set labels, title, and z_lim\n",
    "        ax.set_xlabel('$a_0$ (angle)')\n",
    "        ax.set_ylabel('$a_1$ (magnitude)')\n",
    "        ax.set_title(f'$Q(s_t, a)$ for timestep')\n",
    "        ax.set_zlim(min_z, max_z)\n",
    "\n",
    "        # Convert plot to image\n",
    "        fig.canvas.draw()\n",
    "        frame = np.array(fig.canvas.renderer.buffer_rgba())\n",
    "        frames.append(frame)\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Animate the sampled points rising from the bottom\n",
    "    rise_steps = 20\n",
    "    for t in np.linspace(0, 1, rise_steps):\n",
    "        fig = plt.figure(figsize=(10, 8), dpi=200)\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "        # Plot the Q-approximator surface\n",
    "        ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='viridis', alpha=0.7, edgecolor='none')\n",
    "\n",
    "        # Plot the Lichtenberg figure\n",
    "        figure = np.array(figures[-1])\n",
    "        figure_x = figure[:, 0]\n",
    "        figure_y = figure[:, 1]\n",
    "        figure_z = [min_z for _ in figure]\n",
    "        ax.scatter(figure_x, figure_y, figure_z, color='black', alpha=0.5, s=0.1)\n",
    "\n",
    "        # Animate sampled points rising\n",
    "        sampled_x = [p[0] for p in samples]\n",
    "        sampled_y = [p[1] for p in samples]\n",
    "        sampled_z = [(1 - t) * min_z + t * J.evaluate(p) for p in samples]\n",
    "        ax.scatter(sampled_x, sampled_y, sampled_z, color='blue', alpha=0.8, s=50)\n",
    "\n",
    "        # Plot all previous best points in red\n",
    "        if best_coords_history:\n",
    "            best_x = [p[0] for p in best_coords_history]\n",
    "            best_y = [p[1] for p in best_coords_history]\n",
    "            best_z = [J.evaluate(p) for p in best_coords_history]\n",
    "            ax.scatter(best_x, best_y, best_z, color='red', s=100, edgecolors='black', label='Best Points')\n",
    "            ax.plot(best_x, best_y, best_z, color='red', linestyle='--', label='Best Path')\n",
    "\n",
    "        # Set labels, title, and z_lim\n",
    "        ax.set_xlabel('$a_0$ (angle)')\n",
    "        ax.set_ylabel('$a_1$ (magnitude)')\n",
    "        ax.set_title(f'$Q(s_t, a)$ for timestep')\n",
    "        ax.set_zlim(min_z, max_z)\n",
    "\n",
    "        # Convert plot to image\n",
    "        fig.canvas.draw()\n",
    "        frame = np.array(fig.canvas.renderer.buffer_rgba())\n",
    "        frames.append(frame)\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Highlight the best point\n",
    "    sample_fitnesses = [(J.evaluate(s), s) for s in samples]\n",
    "    max_fitness, max_sample = max(sample_fitnesses, key=lambda x: x[0])\n",
    "    max_samples.append(max_sample)\n",
    "\n",
    "    if max_fitness > best_fitness:\n",
    "        best_fitness = max_fitness\n",
    "        best_coords = max_sample\n",
    "\n",
    "    best_coords_history.append(best_coords)  # Add to history\n",
    "    trigger = best_coords\n",
    "\n",
    "    # Plot the final frame with the best point highlighted\n",
    "    fig = plt.figure(figsize=(10, 8), dpi=200)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Plot the Q-approximator surface\n",
    "    ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='viridis', alpha=0.7, edgecolor='none')\n",
    "\n",
    "    # Plot the Lichtenberg figure\n",
    "    figure = np.array(figures[-1])\n",
    "    figure_x = figure[:, 0]\n",
    "    figure_y = figure[:, 1]\n",
    "    figure_z = [min_z for _ in figure]\n",
    "    ax.scatter(figure_x, figure_y, figure_z, color='black', alpha=0.5, s=0.1)\n",
    "\n",
    "    # Plot sampled points\n",
    "    sampled_x = [p[0] for p in samples]\n",
    "    sampled_y = [p[1] for p in samples]\n",
    "    sampled_z = [J.evaluate(p) for p in samples]\n",
    "    ax.scatter(sampled_x, sampled_y, sampled_z, color='blue', alpha=0.8, s=50)\n",
    "\n",
    "    # Highlight the best point in green\n",
    "    ax.scatter(best_coords[0], best_coords[1], best_fitness, color='green', s=100, edgecolors='black', label='Selected Point')\n",
    "\n",
    "    # Plot all previous best points in red\n",
    "    if best_coords_history:\n",
    "        best_x = [p[0] for p in best_coords_history]\n",
    "        best_y = [p[1] for p in best_coords_history]\n",
    "        best_z = [J.evaluate(p) for p in best_coords_history]\n",
    "        ax.scatter(best_x, best_y, best_z, color='red', s=100, edgecolors='black', label='Best Points')\n",
    "        ax.plot(best_x, best_y, best_z, color='red', linestyle='--', label='Best Path')\n",
    "\n",
    "    # Set labels, title, and z_lim\n",
    "    ax.set_xlabel('$a_0$ (angle)')\n",
    "    ax.set_ylabel('$a_1$ (magnitude)')\n",
    "    ax.set_title(f'$Q(s_t, a)$ for timestep')\n",
    "    ax.set_zlim(min_z, max_z)\n",
    "\n",
    "    # Convert plot to image\n",
    "    fig.canvas.draw()\n",
    "    frame = np.array(fig.canvas.renderer.buffer_rgba())\n",
    "    frames.append(frame)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Add a pause after the points are sampled and the best point is highlighted\n",
    "    for _ in range(20):  # Add 20 frames for pause\n",
    "        frames.append(frame)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8), dpi=200)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='viridis', alpha=0.7, edgecolor='none')\n",
    "ax.scatter(best_coords[0], best_coords[1], best_fitness, color='red', s=150, edgecolors='black', label='Selected Point')\n",
    "ax.set_xlabel('$a_0$ (angle)')\n",
    "ax.set_ylabel('$a_1$ (magnitude)')\n",
    "ax.set_title(f'$Q(s_t, a)$ for timestep')\n",
    "ax.set_zlim(min_z, max_z)\n",
    "fig.canvas.draw()\n",
    "frame = np.array(fig.canvas.renderer.buffer_rgba())\n",
    "frames.append(frame)\n",
    "plt.close(fig)\n",
    "for _ in range(20):  # Add 20 frames for pause\n",
    "    frames.append(frame)\n",
    "\n",
    "to_gif(frames, 'optimization_process.gif', duration=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved as output.mp4\n"
     ]
    }
   ],
   "source": [
    "import imageio.v2 as imageio\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Function to read a GIF and get its frames\n",
    "def read_gif(filename):\n",
    "    reader = imageio.get_reader(filename)\n",
    "    frames = [cv2.cvtColor(np.array(frame), cv2.COLOR_RGBA2BGR) for frame in reader]\n",
    "    return frames\n",
    "\n",
    "# Load GIFs\n",
    "gif1_frames = read_gif(\"simulation_q_function.gif\")\n",
    "gif2_frames = read_gif(\"q_function_transition.gif\")\n",
    "gif3_frames = read_gif(\"optimize1.gif\")\n",
    "\n",
    "# Find the GIF with the lowest resolution\n",
    "min_height = min(frame.shape[0] for gif in [gif1_frames, gif2_frames, gif3_frames] for frame in gif)\n",
    "min_width = min(frame.shape[1] for gif in [gif1_frames, gif2_frames, gif3_frames] for frame in gif)\n",
    "\n",
    "# Function to resize frames to the smallest resolution while keeping the aspect ratio\n",
    "def resize_frame(frame, target_size):\n",
    "    h, w, _ = frame.shape\n",
    "    scale = min(target_size[0] / h, target_size[1] / w)\n",
    "    new_size = (int(w * scale), int(h * scale))\n",
    "    resized_frame = cv2.resize(frame, new_size, interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Centering the frame\n",
    "    pad_top = (target_size[0] - resized_frame.shape[0]) // 2\n",
    "    pad_bottom = target_size[0] - resized_frame.shape[0] - pad_top\n",
    "    pad_left = (target_size[1] - resized_frame.shape[1]) // 2\n",
    "    pad_right = target_size[1] - resized_frame.shape[1] - pad_left\n",
    "    \n",
    "    padded_frame = cv2.copyMakeBorder(resized_frame, pad_top, pad_bottom, pad_left, pad_right, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "    \n",
    "    return padded_frame\n",
    "\n",
    "# Resize all frames\n",
    "target_size = (min_height, min_width)\n",
    "all_frames = []\n",
    "for gif in [gif1_frames, gif2_frames, gif3_frames]:\n",
    "    for frame in gif:\n",
    "        all_frames.append(resize_frame(frame, target_size))\n",
    "\n",
    "# Define video writer\n",
    "output_filename = \"output.mp4\"\n",
    "fps = 30  # Increased frame rate\n",
    "height, width = target_size\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "video_writer = cv2.VideoWriter(output_filename, fourcc, fps, (width, height))\n",
    "\n",
    "# Write frames to video\n",
    "for frame in all_frames:\n",
    "    video_writer.write(frame)\n",
    "\n",
    "video_writer.release()\n",
    "print(\"Video saved as\", output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2026/10000 [02:17<09:02, 14.69it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_209514/3380889005.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDDPGTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muntil_convergence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvergence_reward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_converged\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ISEF2425/Agents/ddpg.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, episodes)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeaturizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ISEF2425/Agents/ddpg.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state, explore)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_approximator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_approximator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrigger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_approximator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ISEF2425/Lichtenberg Optimization/la.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, J, n_iter, pop, minimize, save)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mlf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mlf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ISEF2425/Lichtenberg Optimization/la.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mflat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(1):\n",
    "    # agent = DDPGAgent(state_dim, action_dim, batch_size=32, tau=0.05)\n",
    "    agent = LichtenbergAgent(state_dim, action_dim, \"figure2d.npy\", batch_size=32, tau=0.1, n_iter=3, pop=15)\n",
    "\n",
    "    trainer = DDPGTrainer(env, agent, featurizer, until_convergence=True, convergence_reward=100)\n",
    "    start = time.perf_counter()\n",
    "    trainer.train(episodes=10000)\n",
    "    while not trainer.has_converged(100):\n",
    "        trainer.train(episodes=100)\n",
    "    episodes.append(len(trainer.episode_rewards))\n",
    "    time_taken.append(int(round(time.perf_counter() - start)))\n",
    "    # print(episodes[-1], time_taken[-1])\n",
    "    # print(trainer.episode_rewards, trainer.actor_losses, trainer.critic_losses)\n",
    "    # trainer.plot_losses()\n",
    "    # trainer.plot_rewards()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class ParkingFeaturizer(StandardScaler):\n",
    "    def __init__(self):\n",
    "        super().__init__(19)\n",
    "    \n",
    "    def transform_state(self, state, info=None):\n",
    "        speed = info['speed'] if info else 0.0\n",
    "        state = np.concat([state['observation'], state['achieved_goal'], state['desired_goal'], np.array([speed])])\n",
    "        return torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import highway_env.envs.parking_env as parking_env\n",
    "\n",
    "env = parking_env.ParkingEnv({\n",
    "    \"observation\": {\n",
    "        \"type\": \"KinematicsGoal\",\n",
    "        \"features\": ['x', 'y', 'vx', 'vy', 'cos_h', 'sin_h'],\n",
    "        \"scales\": [100, 100, 5, 5, 1, 1],\n",
    "        \"normalize\": True\n",
    "    },\n",
    "    \"action\": {\n",
    "        \"type\": \"ContinuousAction\"\n",
    "    },\n",
    "    \"simulation_frequency\": 15,\n",
    "    \"policy_frequency\": 3,\n",
    "    \"screen_width\": 600,\n",
    "    \"screen_height\": 300,\n",
    "    \"centering_position\": [0.5, 0.5],\n",
    "    \"scaling\": 7,\n",
    "    \"show_trajectories\": False,\n",
    "    \"render_agent\": True,\n",
    "    \"offscreen_rendering\": True\n",
    "})\n",
    "env.render_mode = 'rgb_array'\n",
    "featurizer = ParkingFeaturizer()\n",
    "agent = LichtenbergAgent(19, 2, \"figure2d.npy\", hidden_layers=3, tau=0.05, batch_size=64, n_iter=3, pop=15)\n",
    "trainer = DDPGTrainer(env, agent, featurizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(episodes=10000)\n",
    "# trainer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('InvertedDoublePendulum-v4', render_mode='rgb_array')\n",
    "env.action_space = np.linspace(env.action_space.low, env.action_space.high, 21) # discretize action space\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "featurizer = StandardScaler(state_dim) # apply adaptive scaling to state vectors\n",
    "agent = DQLAgent( \n",
    "                 input_dim=state_dim, # neural net params\n",
    "                 output_dim=env.action_space.shape[0],\n",
    "                 hidden_dim=128,\n",
    "                 hidden_layers=5,\n",
    "                 batch_size=256,\n",
    "                 gamma=0.99, # discount factor\n",
    "                 min_epsilon=0.1, epsilon_decay=0.999, # exploration rate and decay\n",
    "                 tau=0.005 # update rate of target net\n",
    "                )\n",
    "\n",
    "trainer = DQLTrainer(env, agent, featurizer)\n",
    "trainer.train(episodes=300)\n",
    "info = trainer.run_episode(False)\n",
    "print(f\"cumulative reward: {info['reward']:.2f}, steps: {info['steps']}\")\n",
    "trainer.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('InvertedDoublePendulum-v4', render_mode='rgb_array')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "featurizer = StandardScaler(state_dim)\n",
    "agent = DDPGAgent(state_dim, action_dim, hidden_layers=2, tau=0.01, batch_size=256)\n",
    "featurizer = StandardScaler(state_dim)\n",
    "trainer = DDPGTrainer(env, agent, featurizer)\n",
    "trainer.train(episodes=10000)\n",
    "trainer.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ball and Beam Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ballbeam_gym.envs\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "env = ballbeam_gym.envs.BallBeamSetpointEnv(timestep=0.02, setpoint=-0.8, beam_length=2.0, max_angle=0.5, max_timesteps=500, action_mode='discrete')\n",
    "env.action_space = np.arange(3)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "featurizer = StandardScaler(state_dim)\n",
    "agent = DQLAgent(state_dim, env.action_space.shape[0], batch_size=128, epsilon_decay=0.9995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DQLTrainer(env, agent, featurizer)\n",
    "trainer.train(episodes=1000)\n",
    "trainer.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = trainer.run_episode()\n",
    "to_gif(info['rgb_arrays'], 'ball_and_beam.gif', duration=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Panda Reach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class PandaFeaturizer(StandardScaler):\n",
    "    def __init__(self):\n",
    "        super().__init__(12)\n",
    "    \n",
    "    def transform_state(self, state, info=None):\n",
    "        state = np.concatenate([state['achieved_goal'], state['desired_goal'], state['observation']])\n",
    "        return torch.tensor(state, dtype=torch.float32).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argv[0]=--background_color_red=0.8745098114013672\n",
      "argv[1]=--background_color_green=0.21176470816135406\n",
      "argv[2]=--background_color_blue=0.1764705926179886\n"
     ]
    }
   ],
   "source": [
    "import gymnasium \n",
    "import panda_gym\n",
    "env = gymnasium.make('PandaReach-v3', render_mode=\"rgb_array\")\n",
    "featurizer = PandaFeaturizer()\n",
    "agent = DDPGAgent(12, env.action_space.shape[0])\n",
    "trainer = DDPGTrainer(env, agent, featurizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
